# LINE Bot with Local Ollama VLM Integration

A headless LINE group chatbot backend with keyword-triggered responses (`!hej`, `!img`, `!reload`) that integrates with a locally-deployed Ollama gemma3 4B VLM on RTX 4080 GPU.

**å…·å‚™ Ollama æœ¬æ©Ÿ VLM æ•´åˆçš„ LINE ç¾¤çµ„èŠå¤©æ©Ÿå™¨äºº**ï¼Œæ”¯æ´é—œéµå­—è§¸ç™¼å›æ‡‰ï¼ˆ`!hej`ã€`!img`ã€`!reload`ï¼‰ï¼Œå¯åœ¨ RTX 4080 GPU ä¸ŠåŸ·è¡Œæœ¬æ©Ÿéƒ¨ç½²çš„ Ollama gemma3 4B è¦–è¦ºèªè¨€æ¨¡å‹ã€‚

---

## ğŸ“– Documentation | æ–‡ä»¶

- **ğŸš€ 5 åˆ†é˜å¿«é€Ÿå•Ÿå‹•**ï¼š[QUICKSTART.md](QUICKSTART.md)ï¼ˆä¸­æ–‡ï¼‰
- **ğŸ“š å®Œæ•´è¨­å®šæ•™å­¸**ï¼š[docs/setup-guide-zh-TW.md](docs/setup-guide-zh-TW.md)ï¼ˆä¸­æ–‡ï¼‰
- **ğŸ”§ English Documentation**: See below

---

## Features

- **!hej [question]** - Ask the AI assistant questions, supports multimodal (image analysis) via reply
- **!img [keyword]** - Retrieve predefined images by keyword
- **!reload** - Force refresh configuration from Google Drive

## Architecture

- **Framework**: FastAPI with async request handling
- **LLM**: Ollama with gemma3:4b model (or any Ollama-compatible VLM)
- **Storage**: Google Drive for collaborative prompt/image management
- **Queue**: Async queue with max 10 pending requests, sequential processing (concurrency=1)
- **Rate Limiting**: 30 requests/minute per user
- **Deployment**: Cloudflare Tunnel for public webhook access

## Quick Start

### Prerequisites

- Python 3.11+
- Ollama with gemma3:4b model (`ollama pull gemma3:4b`)
- LINE Messaging API credentials
- (Optional) Google Cloud service account for Drive integration

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd AI-linebot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment template and configure
cp .env.example .env
# Edit .env with your credentials
```

### Configuration

Edit `.env` with your credentials:

```bash
# Required
LINE_CHANNEL_SECRET=your_channel_secret
LINE_CHANNEL_ACCESS_TOKEN=your_access_token

# Optional (for Google Drive integration)
GOOGLE_SERVICE_ACCOUNT_FILE=/path/to/service_account.json
DRIVE_FOLDER_ID=your_drive_folder_id
```

### Running

#### æ–¹æ³•ä¸€ï¼šDocker Composeï¼ˆæ¨è–¦ï¼âœ¨ï¼‰

**ä¸€éµå•Ÿå‹•æ‰€æœ‰æœå‹™ï¼ˆBot + Ollama + Cloudflare Tunnelï¼‰ï¼š**

```bash
# å¿«é€Ÿå•Ÿå‹•ï¼ˆæ¨è–¦ä½¿ç”¨æ­¤è…³æœ¬ï¼‰
./start.sh

# æˆ–æ‰‹å‹•å•Ÿå‹•
docker-compose up -d

# æŸ¥çœ‹ Cloudflare Tunnel URLï¼ˆç”¨æ–¼è¨­å®š LINE Webhookï¼‰
./get-url.sh

# æˆ–æ‰‹å‹•æŸ¥çœ‹
docker logs cloudflared 2>&1 | grep "https://"

# ä¸‹è¼‰æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
docker-compose exec ollama ollama pull gemma3:4b

# æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f linebot

# åœæ­¢æ‰€æœ‰æœå‹™
docker-compose down
```

**é‡è¦ï¼šå°‡é¡¯ç¤ºçš„ Cloudflare URL è¨­å®šåˆ° LINE Developers Console çš„ Webhook URL**

ç¯„ä¾‹ï¼š`https://xxxxx.trycloudflare.com/webhook`

è©³ç´°è¨­å®šè«‹åƒè€ƒ [`docs/setup-guide-zh-TW.md`](docs/setup-guide-zh-TW.md)

#### æ–¹æ³•äºŒï¼šæ‰‹å‹•åŸ·è¡Œï¼ˆé–‹ç™¼æ¨¡å¼ï¼‰

```bash
# çµ‚ç«¯æ©Ÿ 1 - Ollama
ollama serve

# çµ‚ç«¯æ©Ÿ 2 - LINE Bot
python main.py

# çµ‚ç«¯æ©Ÿ 3 - Cloudflare Tunnel
cloudflared tunnel --url http://localhost:8000
```

### Webhook Setup

1. Expose your server via Cloudflare Tunnel or ngrok
2. Set webhook URL in LINE Developers Console: `https://your-domain/webhook`
3. Enable webhook and disable auto-reply in LINE Official Account settings

## Project Structure

```
src/
â”œâ”€â”€ main.py              # FastAPI app, webhook endpoint, background workers
â”œâ”€â”€ config.py            # Environment configuration
â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ llm_request.py   # LLM request queue item
â”‚   â”œâ”€â”€ prompt_config.py # System prompt configuration
â”‚   â”œâ”€â”€ image_mapping.py # Keyword-to-image mappings
â”‚   â”œâ”€â”€ cached_asset.py  # Drive file cache metadata
â”‚   â””â”€â”€ rate_limit.py    # Per-user rate limiting
â”œâ”€â”€ services/            # Business logic
â”‚   â”œâ”€â”€ line_service.py  # LINE API integration
â”‚   â”œâ”€â”€ ollama_service.py # Ollama LLM integration
â”‚   â”œâ”€â”€ drive_service.py # Google Drive sync
â”‚   â”œâ”€â”€ queue_service.py # Async request queue
â”‚   â”œâ”€â”€ image_service.py # In-memory image processing
â”‚   â””â”€â”€ rate_limit_service.py # Rate limiting
â”œâ”€â”€ handlers/            # Command handlers
â”‚   â”œâ”€â”€ command_handler.py # Command parsing and routing
â”‚   â”œâ”€â”€ hej_handler.py   # !hej command
â”‚   â”œâ”€â”€ img_handler.py   # !img command
â”‚   â””â”€â”€ reload_handler.py # !reload command
â””â”€â”€ utils/               # Utilities
    â”œâ”€â”€ logger.py        # Structured logging
    â””â”€â”€ validators.py    # Input validation, security
```

## Google Drive Setup (Optional)

1. Create a folder in Google Drive
2. Add these files to the folder:
   - `system_prompt.md` - System prompt for the AI
   - `image_map.json` - Keyword-to-image mappings
   - `images/` subfolder - Image files referenced in image_map.json

3. Share the folder with your service account email
4. Configure `DRIVE_FOLDER_ID` in `.env`

### image_map.json Example

```json
{
  "mappings": [
    {"keyword": "architecture", "filename": "architecture.png", "file_id": "..."},
    {"keyword": "meme", "filename": "funny_cat.jpg", "file_id": "..."}
  ],
  "version": 1,
  "updated_at": "2026-01-07T12:00:00Z"
}
```

## API Endpoints

- `POST /webhook` - LINE webhook endpoint
- `GET /health` - Health check with service status

## Development

```bash
# Run with auto-reload (development only)
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000

# Run tests
pytest tests/
```

## License

MIT
