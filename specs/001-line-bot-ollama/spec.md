# Feature Specification: LINE Bot with Local Ollama VLM Integration

**Feature Branch**: `001-line-bot-ollama`
**Created**: 2026-01-07
**Status**: Draft
**Input**: User description: "LINE Bot system with keyword-triggered responses, local Ollama gemma3 4B VLM integration, Google Drive collaborative prompt/asset management, reply-based multimodal interactions, and async queue for GPU resource management"

## Clarifications

### Session 2026-01-07

- Q: Should Google Drive changes be automatically detected and applied, or only loaded when users manually trigger `!reload`? → A: Automatic polling - Bot periodically checks Google Drive (e.g., every 30-60 seconds) and auto-applies changes
- Q: How should the system notify administrators when critical services fail? → A: LINE group notification - Post error messages to designated administrators via LINE (not in the main group to avoid noise)
- Q: What is the maximum queue size before new requests are rejected? → A: 10 requests
- Q: What is the maximum image dimension (width/height) before automatic resizing? → A: 1920 pixels (longest side)
- Q: What is the maximum number of `!ask` requests per user within a time window? → A: 30 requests per minute
- Q: What command keyword should trigger LLM inference? → A: `!hej` (replacing `!ask`)
- Q: Should downloaded images from LINE be stored locally? → A: No - stream directly to LLM without local storage for privacy protection
- Q: What backend framework should be used? → A: FastAPI
- Q: What performance characteristics are prioritized? → A: Maximum efficiency, optimal performance, fastest speed, minimal memory usage

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Text-Based AI Conversation (Priority: P1)

Users in a LINE group can ask the bot questions using a specific command, and receive intelligent responses powered by a locally-hosted vision-language model.

**Why this priority**: This is the core value proposition - enabling AI-powered conversations in LINE groups without relying on external cloud services or APIs. This story alone delivers a functional chatbot.

**Independent Test**: Can be fully tested by sending `!hej [question]` in a LINE group and receiving an AI-generated response. Delivers immediate value as a working chatbot without requiring any other features.

**Acceptance Scenarios**:

1. **Given** a user is in a LINE group with the bot, **When** they send `!hej What is the capital of France?`, **Then** the bot responds with an answer generated by the local LLM
2. **Given** multiple users send `!hej` commands simultaneously, **When** the bot processes them, **Then** requests are queued and processed sequentially without system crashes
3. **Given** a user sends a regular message without the `!hej` prefix, **When** the bot sees this message, **Then** the bot remains silent and does not respond

---

### User Story 2 - Multimodal Conversation via Reply (Priority: P1)

Users can ask the bot to analyze images or quoted text by replying to existing messages with the `!hej` command, enabling contextual AI interactions.

**Why this priority**: This leverages the vision capabilities of the VLM and provides unique value beyond text-only bots. It's a key differentiator and can be tested independently by replying to messages.

**Independent Test**: Can be fully tested by (1) someone posting an image or text in the group, (2) another user replying to that message with `!hej [question about the content]`, and (3) the bot analyzing the referenced content. Delivers value as an image/text analysis tool.

**Acceptance Scenarios**:

1. **Given** a message with an image exists in the group, **When** a user replies to it with `!hej What's in this image?`, **Then** the bot downloads the image, streams it directly to the VLM without local storage, and returns the analysis
2. **Given** a message with text exists in the group, **When** a user replies to it with `!hej Summarize this`, **Then** the bot extracts the quoted text and sends it to the LLM for processing
3. **Given** a user replies to a message with `!hej` but the referenced message contains neither text nor image, **When** the bot processes the request, **Then** the bot responds with an error message explaining no analyzable content was found

---

### User Story 3 - Collaborative Prompt Engineering (Priority: P2)

Non-technical team members can edit the bot's system prompts and behavior by updating Markdown files in a shared Google Drive folder, with changes automatically detected and applied within 30-60 seconds.

**Why this priority**: This enables collaborative iteration on bot behavior without code changes. While valuable, the bot can function with default prompts, making this a P2 feature.

**Independent Test**: Can be fully tested by (1) editing `system_prompt.md` in Google Drive, (2) waiting up to 60 seconds, and (3) observing different bot behavior based on the new prompt in subsequent `!hej` responses. Delivers value as a no-code customization system.

**Acceptance Scenarios**:

1. **Given** a user has edit access to the Google Drive folder, **When** they modify `system_prompt.md`, **Then** the bot automatically detects the change within 60 seconds and subsequent responses reflect the new prompt
2. **Given** the Google Drive sync fails during automatic polling, **When** the bot attempts to check for updates, **Then** the bot continues using the last successfully loaded prompt and logs the sync failure
3. **Given** the prompt file contains invalid syntax when detected during automatic sync, **When** the bot attempts to apply changes, **Then** the bot logs a warning and continues using the previous valid prompt

---

### User Story 4 - Image Asset Retrieval (Priority: P3)

Users can request predefined images (like architecture diagrams, guides, or memes) using keyword commands, with the mapping managed collaboratively via Google Drive.

**Why this priority**: This is a convenience feature for sharing common resources. The bot's core AI functionality doesn't depend on it, making it P3.

**Independent Test**: Can be fully tested by (1) configuring `image_map.json` in Google Drive with a keyword-to-filename mapping, (2) sending `!img [keyword]` in LINE, and (3) receiving the corresponding image. Delivers value as a shared image library.

**Acceptance Scenarios**:

1. **Given** `image_map.json` maps "架構圖" to "architecture.png", **When** a user sends `!img 架構圖`, **Then** the bot downloads the image from Google Drive (if not cached locally) and sends it to the LINE group
2. **Given** a user requests a keyword that doesn't exist in the mapping, **When** they send `!img unknown`, **Then** the bot responds with an error message listing available keywords
3. **Given** an image file referenced in the mapping is missing from Google Drive, **When** a user requests it, **Then** the bot responds with an error and notifies administrators

---

### Edge Cases

- What happens when the Ollama service is offline or unresponsive?
  - Bot should respond with a friendly error message and log the incident for monitoring
- How does the system handle extremely long prompts that exceed the model's context window?
  - Bot should truncate the input intelligently (e.g., keep most recent context) and warn the user
- What happens when Google Drive updates occur during active LLM inference?
  - System should apply changes only after current inference completes to maintain consistency
- How does the bot behave when Google Drive authentication expires?
  - Bot should continue functioning with cached data and alert administrators about authentication failure
- What happens when a user replies to a message chain (reply to a reply)?
  - Bot should extract content from the immediate parent message only, not traverse the entire chain
- How does the system handle very large images that might cause memory issues?
  - Images exceeding 1920px on longest side should be automatically resized while preserving aspect ratio before sending to VLM
- What happens when the async queue fills up during high traffic?
  - New requests should be rejected with a "busy" message when queue reaches 10 pending requests
- How does the bot handle non-UTF8 encoded content or special characters in prompts/images?
  - System should handle encoding gracefully or respond with appropriate error messages

## Requirements *(mandatory)*

### Functional Requirements

#### Command Processing
- **FR-001**: System MUST only respond to messages that start with specific command prefixes (`!hej`, `!img`, `!reload`)
- **FR-002**: System MUST ignore all other messages in the group to avoid unwanted interactions
- **FR-003**: System MUST parse `!hej <question>` commands and extract the question text
- **FR-004**: System MUST parse `!img <keyword>` commands and extract the keyword
- **FR-005**: System MUST support manual `!reload` command to force immediate refresh of prompts and configuration from Google Drive (supplementing automatic sync)

#### LLM Integration
- **FR-006**: System MUST integrate with locally deployed Ollama service running gemma3 4B VLM model
- **FR-007**: System MUST send text prompts to Ollama API in the correct payload format
- **FR-008**: System MUST resize images to maximum 1920px on longest side (preserving aspect ratio) in-memory without saving to disk
- **FR-009**: System MUST stream images directly to Ollama API without local file storage for privacy protection
- **FR-010**: System MUST encode images to base64 in-memory and send to Ollama API when processing image content
- **FR-011**: System MUST combine system prompts from Google Drive with user input before sending to the LLM
- **FR-012**: System MUST handle LLM response streaming or blocking responses appropriately

#### Reply-Based Interaction
- **FR-013**: System MUST detect when a user replies to an existing message with `!hej`
- **FR-014**: System MUST extract text content from the referenced message when available
- **FR-015**: System MUST download and extract image content from the referenced message when available (stream to LLM without local storage)
- **FR-016**: System MUST support multimodal requests (combining text question in reply with image/text from original message)

#### Google Drive Synchronization
- **FR-017**: System MUST authenticate with Google Drive using service account or OAuth credentials
- **FR-018**: System MUST maintain a local cache of Google Drive files to reduce API calls
- **FR-019**: System MUST download `system_prompt.md` from the designated Google Drive folder
- **FR-020**: System MUST download `image_map.json` configuration from Google Drive
- **FR-021**: System MUST download image assets from the Google Drive `images/` folder on demand
- **FR-022**: System MUST automatically check Google Drive for file modifications every 30-60 seconds using background polling
- **FR-023**: System MUST detect changes by comparing file modification timestamps or checksums against cached versions
- **FR-024**: System MUST apply detected Google Drive changes automatically without requiring user intervention
- **FR-025**: System MUST reload Google Drive content immediately when the `!reload` command is issued (manual override of automatic sync)
- **FR-026**: System MUST validate JSON configuration files before applying changes from either automatic sync or manual reload

#### Image Management
- **FR-027**: System MUST map keywords to image filenames using the `image_map.json` configuration
- **FR-028**: System MUST send images directly to LINE using binary image data from Google Drive cache
- **FR-029**: System MUST cache downloaded images locally to avoid repeated Drive downloads

#### Asynchronous Queue System
- **FR-031**: System MUST queue all LLM inference requests in an async queue with maximum capacity of 10 pending requests
- **FR-032**: System MUST process LLM requests sequentially using a semaphore with concurrency limit of 1
- **FR-033**: System MUST prevent GPU memory overload by limiting concurrent inference jobs
- **FR-034**: System MUST provide feedback to users when their request is queued, including estimated wait time
- **FR-035**: System MUST reject new requests with a "busy" message when queue is full (10 requests)
- **FR-036**: System MUST handle queue timeouts gracefully if requests take too long
- **FR-037**: System MUST optimize queue implementation for minimal memory footprint and maximum throughput

#### LINE Integration
- **FR-038**: System MUST receive webhook events from LINE Messaging API
- **FR-039**: System MUST validate LINE webhook signatures for security
- **FR-040**: System MUST respond to messages using LINE's reply API
- **FR-041**: System MUST handle LINE message types: text and image
- **FR-042**: System MUST download image content from LINE's content API using message IDs

#### Error Handling & Monitoring
- **FR-043**: System MUST log all errors with sufficient detail for debugging
- **FR-044**: System MUST respond to users with friendly error messages when operations fail
- **FR-045**: System MUST continue operating with cached data if Google Drive is temporarily unavailable
- **FR-046**: System MUST send alert notifications to designated administrator LINE user IDs when critical services (Ollama, Drive) are down
- **FR-047**: System MUST NOT post error/alert messages to the main user group to avoid notification noise
- **FR-048**: System MUST validate all user inputs to prevent injection attacks or malformed requests
- **FR-049**: System MUST enforce per-user rate limiting of maximum 30 `!hej` requests per minute per LINE user ID
- **FR-050**: System MUST respond with a rate limit error message when users exceed the limit, indicating time until reset

#### Performance & Efficiency
- **FR-051**: System MUST use FastAPI framework for the backend server implementation
- **FR-052**: System MUST optimize all operations for maximum speed and minimal memory usage
- **FR-053**: System MUST avoid unnecessary data copying or buffering in the request pipeline
- **FR-054**: System MUST use efficient async/await patterns throughout the codebase

### Key Entities

- **LLM Request**: Represents a queued inference job containing user question, context (text/image), system prompt, and metadata (user ID, timestamp, priority)
- **Prompt Configuration**: The system prompt loaded from Google Drive that defines the bot's personality and behavior
- **Image Mapping**: The keyword-to-filename associations defined in `image_map.json`
- **Cached Asset**: Locally stored copies of Google Drive files (prompts, images, config) with timestamps for cache invalidation
- **Message Context**: The referenced message content (text or image) when a user uses reply-based interaction
- **Command**: Parsed user command containing the command type (!ask, !img, !reload) and associated parameters
- **Administrator Contact**: Configured LINE user IDs designated to receive critical system alerts and error notifications

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Users receive LLM responses within 30 seconds of sending `!hej` commands under normal load
- **SC-002**: System handles at least 10 concurrent users sending requests without crashing or data corruption
- **SC-003**: Bot successfully processes multimodal requests (text + image analysis) with 95% success rate
- **SC-004**: Non-technical users can modify bot behavior by editing Google Drive files without developer assistance
- **SC-005**: System maintains 99% uptime for core functionality (command processing, LLM inference) over a one-week period
- **SC-006**: Prompt changes are automatically detected and applied within 60 seconds of modification in Google Drive
- **SC-007**: Image retrieval via `!img` commands completes within 10 seconds including Drive download time
- **SC-008**: Zero GPU out-of-memory errors occur during normal operation with the async queue system
- **SC-009**: Bot correctly identifies and processes 100% of valid commands while ignoring non-command messages
- **SC-010**: System recovers automatically from temporary service outages (Ollama restart, Drive API limits) within 60 seconds
- **SC-011**: Downloaded images are never persisted to local disk, only processed in-memory
- **SC-012**: System maintains minimal memory footprint with efficient resource utilization throughout operation

## Dependencies & Assumptions *(mandatory)*

### Dependencies

- **External Services**:
  - LINE Messaging API for receiving webhook events and sending messages
  - Google Drive API for collaborative file management
  - Ollama service running locally on the same machine as the bot server

- **Hardware**:
  - NVIDIA RTX 4080 GPU with 12GB VRAM for running gemma3 4B VLM
  - Sufficient disk space for caching Google Drive assets locally

- **Network**:
  - Public internet access for LINE webhook callbacks and Google Drive API
  - Local network access to Ollama service (typically localhost:11434)

### Assumptions

- The Ollama service is already installed and the gemma3:4b model is already downloaded
- The bot server has a publicly accessible URL for LINE webhook callbacks (via tunneling service or proper hosting)
- Google Drive folder structure exists with correct permissions set before first run
- LINE Bot account and channel are already created with Messaging API enabled
- Users have appropriate permissions to edit files in the shared Google Drive folder
- The bot operates in a trusted environment (private LINE groups) and users are not adversarial
- Network latency to Google Drive and LINE APIs is reasonable (< 500ms)
- System administrators can manually restart services if critical failures occur
- The gemma3 4B model's context window is sufficient for expected use cases (typically 8K-32K tokens)
- Image assets in Google Drive are in formats supported by both LINE and the VLM (PNG, JPEG)
- The Python asyncio event loop can handle webhook concurrency without additional process/thread management

### Out of Scope

- Multi-language support for bot commands (initially Chinese/English only based on examples)
- User authentication or permission levels within LINE groups
- Persistent conversation history or memory across sessions
- Fine-tuning or customization of the Ollama model itself
- Advanced analytics or usage tracking dashboards
- Support for other LLM providers beyond Ollama
- Video or audio content processing
- Real-time collaborative editing interface for prompts (relies on Google Drive's existing collaboration)
- Automatic model switching or A/B testing of different prompts

## Constraints

- **Framework**: Backend MUST be implemented using FastAPI framework
- **Hardware**: Must run on a single machine with RTX 4080 GPU (12GB VRAM limit)
- **Concurrency**: Only 1 LLM inference request at a time to prevent GPU memory exhaustion
- **Latency**: Google Drive API rate limits may introduce delays during `!reload` or `!img` operations
- **Privacy**: All LLM inference happens locally, no data sent to external AI services. Downloaded images must never be persisted to disk
- **Deployment**: Bot must be accessible via public URL for LINE webhooks (requires Cloudflare Tunnel or similar tunneling solution)
- **Model Limitations**: gemma3 4B VLM capabilities and context window size constrain response quality and length
- **LINE API Limits**: Message sending rate limits and file size restrictions apply
- **Performance Priority**: All implementation decisions must prioritize efficiency, speed, and minimal memory usage

## Security & Privacy Considerations

- **Webhook Validation**: All LINE webhook requests must be validated using signature verification to prevent spoofing
- **Input Sanitization**: User input in `!hej` commands must be sanitized to prevent prompt injection attacks
- **Access Control**: Google Drive service account credentials must be securely stored and not exposed in code
- **Data Retention**: Define policy for how long queued requests and cached assets are retained
- **Error Disclosure**: Error messages should not leak sensitive system information (file paths, API keys)
- **Rate Limiting**: System enforces 30 requests per minute per user to prevent abuse of the `!hej` command
- **Privacy Protection**: Downloaded images from LINE are never stored on local disk - all image processing occurs in-memory only
- **Image Safety**: Consider content filtering for user-uploaded images to prevent harmful content processing
- **Local Inference**: All AI processing happens on local hardware, ensuring user data never leaves the deployment environment
